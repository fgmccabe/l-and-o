lo.comp.grammar{
  import lo.
  import lo.uri.
  import lo.resources.

  import lo.comp.abstract.
  import lo.comp.ast.
  import lo.comp.errors.
  import lo.comp.lexer.
  import lo.comp.location.
  import lo.comp.operators.
  import lo.comp.regexp.
  import lo.comp.token.

  public parseFile:(uri,report,report) => ast.
  parseFile(U,Rp,Rpx) => Term :-
    parse(Term,Rp,Rp0) %% tokenizeFile(U) ~ Rest,
    (Rest=[],Rp0=Rpx | Rest=[Tk,.._],reportError("extra tokens found in file",Tk.loc,Rp0,Rpx)).

  public parseSrc:(uri,list[integer],report,report) => ast.
  parseSrc(U,Codes,Rp,Rpx) => Term :-
    parse(Term,Rp,Rpx) %% tokenCodes(Codes,getUriPath(U)).

  public nextTerm:(list[token],ast,list[token],report,report){}.
  nextTerm(Toks,Term,Rest,Rp,Rpx) :-
    parse(Term,Rp,Rpx) %% Toks ~ Rest.

  tokMark ::= endBrce | otherMark.

  public
  parse:(ast,report,report) --> list[token].
  parse(Term,Rp,Rpx) --> term(2000,Term,Mark,Rp,Rp0)!,
    checkForTerminator(Mark,Rp0,Rpx).

  public term:(integer,ast,tokMark,report,report) --> list[token].
  term(Pr,Term,Mark,Rp,Rpx) -->
    termLeft(Pr,Left,LftPr,LLend,Rp,Rp0),
    termRight(Pr,Left,Term,LftPr,LLend,Mark,Rp0,Rpx).

  private
  termLeft:(integer,ast,integer,tokMark,report,report) --> list[token].
  termLeft(Pr,Left,OPr,Mark,Rp,Rpx) -->
    [tok(idTok(Op),Lc)], \+[tok(rpar,_)], { prefixOp(Op,OPr,ORight), OPr=<Pr},
    term(ORight,Arg,Mark,Rp,Rpx),
    Left=unary(Lc.merge(Arg.loc),Op,Arg).
  termLeft(_,Term,0,Mark,Rp,Rpx) --> term0(Term,Mark,Rp,Rpx).

  private
  termRight:(integer,ast,ast,integer,tokMark,tokMark,report,report) --> list[token].
  termRight(Pr,Left,Term,LeftPr,_,Mark,Rp,Rpx) -->
    legalInfixOp(Op,Pr,LeftPr,InfPr,RightPr),
    term(RightPr,Right,LMark,Rp,Rp0),
    M = binary(Left.loc.merge(Right.loc),Op,Left,Right),
    termRight(Pr,M,Term,InfPr,LMark,Mark,Rp0,Rpx).
  termRight(Pr,Left,Term,LeftPr,_,Mark,Rp,Rpx) -->
    [tok(idTok(Op),Lc)],
    { postfixOp(Op,LPr,PostPr),
        PostPr =< Pr,
        LPr >= LeftPr},
    M = unary(Left.loc.merge(Lc),Op,Left),
    termRight(Pr,M,Term,PostPr,otherMark,Mark,Rp,Rpx).
  termRight(_,Term,Term,_,Mark,Mark,Rp,Rp) --> [].

  private
  legalInfixOp:(string,integer,integer,integer,integer) --> list[token].
  legalInfixOp(Op,Pr,LeftPr,OPr,RPr) -->
      [tok(idTok(Op),_)],
      { infixOp(Op,LPr,OPr,RPr),
        OPr =< Pr,
        LPr >= LeftPr },
      followsInfix(RPr)+.

  private
  followsInfix:(integer) --> list[token].
  followsInfix(Pr) -->
    [tok(idTok(Op),_)] ,
    { prefixOp(Op,OpPr,_) ? OpPr =< Pr | \+ isOperator(Op,_)}.
  followsInfix(_) --> [tok(lpar,_)].
  followsInfix(_) --> [tok(lbra,_)].
  followsInfix(_) --> [tok(lbrce,_)].
  followsInfix(_) --> [tok(lqpar,_)].
  followsInfix(_) --> [tok(stringTok(_),_)].
  followsInfix(_) --> [tok(idQTok(_),_)].
  followsInfix(_) --> [tok(intTok(_),_)].
  followsInfix(_) --> [tok(fltTok(_),_)].
  followsInfix(_) --> [tok(regTok(_),_)].

  private
  term0:(ast,tokMark,report,report) --> list[token].
  term0(parseString(Lc,Segments,Rp,Rpx),otherMark,Rp,Rpx) --> [tok(stringTok(Segments),Lc)].
  term0(intg(Lc,Ix),otherMark,Rp,Rp) --> [tok(intTok(Ix),Lc)].
  term0(flot(Lc,Dx),otherMark,Rp,Rp) --> [tok(fltTok(Dx),Lc)].
  term0(tupl(Lc,"[]",[]),otherMark,Rp,Rp) -->
    [tok(lbra,Lc0),tok(rbra,Lc1)],
    Lc = Lc0.merge(Lc1).
  term0(tupl(Lc,"[]",tupleize(Seq)),otherMark,Rp,Rpx) -->
    [tok(lbra,Lc0)],
    term(2000,Seq,_,Rp,Rp0),
    checkFor(rbra,Lc1,"missing close bracket",Rp0,Rpx),
    Lc = Lc0.merge(Lc1).
  term0(tupl(Lc,"{}",[]),endBrce,Rp,Rp) -->
    [tok(lbrce,Lc0),tok(rbrce,Lc1)],
    Lc = Lc0.merge(Lc1).
  term0(tupl(Lc,"{}",Seq),endBrce,Rp,Rpx) -->
    [tok(lbrce,Lc0)],
    terms(Seq,Rp,Rp0),
    checkFor(rbrce,Lc1,"missing close brace",Rp0,Rpx),
    Lc = Lc0.merge(Lc1).
  term0(unary(Lc,"<||>",A),otherMark,Rp,Rpx) -->
    [tok(lqpar,Lc0)],
    term(2000,A,_,Rp,Rp0),
    checkFor(rqpar,Lc1,"missing close quote",Rp0,Rpx),
    Lc = Lc0.merge(Lc1).
  term0(Term,Mark,Rp,Rpx) -->
    term00(Left,LMark,Rp,Rp0),
    termArgs(Left,Term,LMark,Mark,Rp0,Rpx).

  private
  term00:(ast,tokMark,report,report) --> list[token].
  term00(iden(Lc,Nm),otherMark,Rp,Rp) --> [tok(idTok(Nm),Lc)].
  term00(iden(Lc,Nm),otherMark,Rp,Rp) --> [tok(idQTok(Nm),Lc)].
  term00(tupl(Lc,"()",[]),otherMark,Rp,Rp) -->
      [tok(lpar,Lc0),tok(rpar,Lc1)],
      Lc = Lc0.merge(Lc1).
  term00(tupl(Lc,"()",tupleize(Seq)),otherMark,Rp,Rpx) -->
      [tok(lpar,Lc0)],
      term(2000,Seq,_,Rp,Rp0),
      checkFor(rpar,Lc1,"missing close paren",Rp0,Rpx),
      Lc = Lc0.merge(Lc1).

  private
  termArgs:(ast,ast,tokMark,tokMark,report,report) --> list[token].
  termArgs(Lft,Term,_,Mark,Rp,Rpx) -->
      [tok(lpar,Lc0), tok(rpar,Lc1)],
      Lc = Lft.loc.merge(Lc1),
      termArgs(appl(Lc,Lft,tupl(Lc0.merge(Lc1),"()",[])),Term,otherMark,Mark,Rp,Rpx).
  termArgs(Lft,Term,_,Mark,Rp,Rpx) -->
      [tok(lpar,Lc0)],
      term(2000,Seq,_,Rp,Rp0),
      checkFor(rpar,Lc1,"missing close paren",Rp0,Rp1),
      termArgs(appl(Lft.loc.merge(Lc1),Lft,tupl(Lc0.merge(Lc1),"()",tupleize(Seq))),Term,otherMark,Mark,Rp1,Rpx).
  termArgs(Lft,Term,_,Mark,Rp,Rpx) -->
      [tok(lbra,Lc0), tok(rbra,Lc1)],
      termArgs(appl(Lft.loc.merge(Lc1),Lft,tupl(Lc0.merge(Lc1),"[]",[])),Term,otherMark,Mark,Rp,Rpx).
  termArgs(Lft,appl(Lc,Lft,tupl(TpLc,"[]",tupleize(Seq))),_,otherMark,Rp,Rpx) -->
      [tok(lbra,Lc0)],
      term(2000,Seq,_,Rp,Rp0),
      checkFor(rbra,Lc1,"missing close bracket",Rp0,Rpx),
      TpLc = Lc0.merge(Lc1),
      Lc = Lft.loc.merge(Lc1).
  termArgs(Lft,appl(Lc,Lft,tupl(TpLc,"{}",[])),_,endBrce,Rp,Rp) -->
      [tok(lbrce,Lc0), tok(rbrce,Lc1)],
      Lc = Lft.loc.merge(Lc1),
      TpLc = Lc0.merge(Lc1).
  termArgs(Lft,appl(Lc,Lft,tupl(TpLc,"{}",Els)),_,endBrce,Rp,Rpx) -->
      [tok(lbrce,Lc0)],
      terms(Els,Rp,Rp0),
      checkFor(rbrce,Lc1,"missing close brace",Rp0,Rpx),
      Lc = Lft.loc.merge(Lc1),
      TpLc = Lc0.merge(Lc1).
  termArgs(Lft,Term,_,Mark,Rp,Rpx) -->
      [tok(idTok("."),Lc), tok(idTok(Fld),LcF)],
      termArgs(binary(Lft.loc.merge(LcF),".",Lft,iden(LcF,Fld)),Term,otherMark,Mark,Rp,Rpx).
  termArgs(Term,Term,Mark,Mark,Rp,Rp) --> [].

  private
  terms: (list[ast],report,report) --> list[token].
  terms([T,..R],Rp,Rpx) --> parse(T,Rp,Rp0), terms(R,Rp0,Rpx).
  terms([],Rp,Rp) --> [].

  private
  tupleize:(ast)=>list[ast].
  tupleize(Term) => [L,..tupleize(R)] :- isBinary(Term,",",_,L,R).
  tupleize(T) => [T].

  private
  checkFor:(tok,location,string,report,report) --> list[token].
  checkFor(Tk,Lc,_,Rp,Rp) --> [tok(Tk,Lc)].
  checkFor(Tk,Lc,Msg,Rp,Rpx) --> [tok(_,Lc)]+,
    { reportError(Msg,Lc,Rp,Rpx) }.

  private
  checkForTerminator:(tokMark,report,report) --> list[token].
  checkForTerminator(_,Rp,Rp) --> [tok(period,_)].
  checkForTerminator(_,Rp,Rp) --> eof.
  checkForTerminator(_,Rp,Rp) --> [tok(rbrce,_)]+.
  checkForTerminator(endBrce,Rp,Rp) --> [].
  checkForTerminator(_,Rp,Rpx) -->
    [tok(_,Lc)]+,
    checkFor(period,Lc,"missing terminator",Rp,Rpx).

  private
  parseString:(location,list[stringSegment],report,report) => ast.
  parseString(Lc,[],Rp,Rp) => strg(Lc,"").
  parseString(_,[segment(Lc,Str)],Rp,Rp) => strg(Lc,Str).
  parseString(Lc,Segments,Rp,Rpx) =>
    binary(Lc,"::",unary(Lc,"ssSeq",tupl(Lc,"[]",parseSegments(Segments,Rp,Rpx))),iden(Lc,"string")).

  parseSegments:(list[stringSegment],report,report) => list[ast].
  parseSegments([],Rp,Rp) => [].
  parseSegments([segment(Lc,Str),..More],Rp,Rpx) =>
    [unary(Lc,"ss",strg(Lc,Str)),..parseSegments(More,Rp,Rpx)].
  parseSegments([interpolate(Lc,Exp,Fmt),..More],Rp,Rpx) =>
    [interpolateSegment(Lc,Exp,Fmt,Rp,Rp0),..parseSegments(More,Rp0,Rpx)].

  interpolateSegment:(location,string,string,report,report) => ast.
  interpolateSegment(Lc,Text,"",Rp,Rpx) => unary(Lc,"disp",Arg) :-
    term(2000,Arg,_,Rp,Rpx) %% subTokenize(Lc,Text).
  interpolateSegment(Lc,Text,Fmt,Rp,Rpx) => binary(Lc,"frmt",Arg,strg(Lc,Fmt)) :-
    term(2000,Arg,_,Rp,Rpx) %% subTokenize(Lc,Text).
}
